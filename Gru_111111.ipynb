{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-AZBgXP4wNxa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["49/49 [==============================] - 61s 1s/step - loss: 2.8635 - accuracy: 0.2082\n","Epoch 2/10\n","49/49 [==============================] - 55s 1s/step - loss: 2.2961 - accuracy: 0.3263\n","Epoch 3/10\n","49/49 [==============================] - 53s 1s/step - loss: 1.9144 - accuracy: 0.4154\n","Epoch 4/10\n","49/49 [==============================] - 53s 1s/step - loss: 1.6767 - accuracy: 0.4884\n","Epoch 5/10\n","49/49 [==============================] - 53s 1s/step - loss: 1.4752 - accuracy: 0.5512\n","Epoch 6/10\n","49/49 [==============================] - 53s 1s/step - loss: 1.2568 - accuracy: 0.6001\n","Epoch 7/10\n","49/49 [==============================] - 53s 1s/step - loss: 1.0604 - accuracy: 0.6651\n","Epoch 8/10\n","49/49 [==============================] - 52s 1s/step - loss: 0.8043 - accuracy: 0.7423\n","Epoch 9/10\n","49/49 [==============================] - 55s 1s/step - loss: 0.5925 - accuracy: 0.8105\n","Epoch 10/10\n","16/49 [========\u003e.....................] - ETA: 35s - loss: 0.4246 - accuracy: 0.8770"]}],"source":["\n","\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import re\n","import os\n","\n","with open('los3.txt', 'r') as f:\n","    text = f.read()\n","\n","# Convertir el texto en minúsculas\n","text = text.lower()\n","\n","# Eliminar caracteres especiales y números\n","text = re.sub(r'[^a-zñáéíóúü]', ' ', text)\n","\n","# Crear un diccionario de caracteres únicos y asignar un número a cada uno\n","char_to_idx = {char: idx for idx, char in enumerate(sorted(set(text)))}\n","\n","# Crear el diccionario inverso\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","# Convertir el texto a una lista de números\n","text_as_int = [char_to_idx[char] for char in text]\n","\n","# Longitud de secuencia para la entrada del modelo\n","seq_length = 32\n","\n","# Número de ejemplos de entrenamiento por lote\n","batch_size = 64\n","\n","# Tamaño del vocabulario\n","vocab_size = len(char_to_idx)\n","\n","# Número de unidades en la capa GRU\n","rnn_units = 512\n","\n","# Crear una matriz de entrada y salida para el modelo\n","def create_dataset(text_as_int):\n","    inputs = []\n","    outputs = []\n","\n","    for i in range(len(text_as_int) - seq_length):\n","        inputs.append(np.expand_dims(text_as_int[i:i+seq_length], axis=1))\n","        outputs.append(text_as_int[i+seq_length])\n","\n","    return np.array(inputs), np.array(outputs)\n","\n","# Crear el conjunto de datos de entrenamiento\n","inputs, outputs = create_dataset(text_as_int)\n","\n","vocab_size\n","\n","rnn_units\n","\n","# Crear el modelo\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, rnn_units),\n","    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n","    tf.keras.layers.GRU(rnn_units),\n","    tf.keras.layers.Dense(vocab_size, activation='softmax')\n","])\n","\n","# Función de pérdida\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","# Compilar el modelo\n","model.compile(optimizer='adam', loss=loss,metrics=['accuracy'])\n","\n","# Crear un punto de control para guardar el modelo\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","\n","# Entrenar el modelo\n","history = model.fit(inputs, outputs, batch_size=batch_size, epochs=10, callbacks=[checkpoint_callback])\n","\n","# Guardar el modelo en un archivo hdf5\n","model.save('modfi.h5')"]},{"cell_type":"markdown","metadata":{"id":"yNIh48NdD2lm"},"source":["#Generar Texto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eG-r9sgs4mY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6R3GiDOD1WH"},"outputs":[],"source":["def generate_text(model, start_string, num_generate=10, temperature=1.0):\n","    string = start_string.lower()\n","    print(string)\n","    string = re.sub(r'[^a-zñáéíóúü]', ' ', string)\n","    input_eval = [char_to_idx[char] for char in string]\n","    if len(input_eval) \u003c seq_length:\n","        input_eval = tf.pad(input_eval, [(0, seq_length - len(input_eval))], mode='CONSTANT', constant_values=0)\n","    input_eval = tf.reshape(input_eval, (1, seq_length))\n","\n","    # Crear una lista vacía para almacenar el texto generado\n","    text_generated = []\n","\n","    # Establecer la temperatura\n","    model.reset_states()\n","    temperature = temperature\n","\n","    for i in range(num_generate):\n","        # Predecir el siguiente carácter utilizando el modelo\n","        predictions = model(input_eval)\n","       \n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","       \n","        # Añadir el carácter generado a la lista de texto generado\n","        text_generated.append(idx_to_char[predicted_id])\n","        \n","\n","        # Utilizar el carácter generado como entrada para la siguiente predicción\n","        input_eval = tf.concat([input_eval[:, 1:], tf.expand_dims([predicted_id], 1)], axis=-1)\n","\n","\n","\n","    return ( ''.join(text_generated))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3Z4KMnAD1RI"},"outputs":[],"source":["# Generar texto nuevo\n","generated_text = generate_text(model, start_string='cerditos', num_generate=10, temperature=0.5)\n","print()\n","# Imprimir el texto generado\n","print(generated_text)"]}],"metadata":{"colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}