{"cells":[{"cell_type":"markdown","metadata":{"id":"rB8g5Zr-pSWi"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4231,"status":"ok","timestamp":1683653382982,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"iRjBk9bWr5eg"},"outputs":[],"source":["import tensorflow\n","from tensorflow.python.keras.layers import Layer, InputSpec"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1782,"status":"ok","timestamp":1683653384761,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"8IJTXLTdTFV1"},"outputs":[],"source":["import re\n","import os\n","import random\n","import numpy as np\n","import seaborn as sn\n","import pandas as pd\n","import bokeh.plotting as bp\n","from bokeh.models import HoverTool, BoxSelectTool\n","from bokeh.plotting import figure, show, output_notebook\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical, get_file\n","\n","from keras.layers import Input, LSTM, Dense, TimeDistributed\n","from keras.layers import Embedding, Concatenate, Dropout\n","from keras.models import Model\n","from keras import initializers\n","import keras.backend as K\n","\n","checkpoint_dir = 'checkpoints'"]},{"cell_type":"markdown","metadata":{"id":"fHOr5kTvfcIM"},"source":["### Conectar con Google Drive"]},{"cell_type":"markdown","metadata":{"id":"aAWJoWN7pSWk"},"source":["### Connect with Google Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8678,"status":"ok","timestamp":1683653393434,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"5cuUp8-sT5N9"},"outputs":[],"source":["try:  # estamos en Google Colab?\n","      # are we in Google Colab?\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    checkpoint_dir = '/content/drive/My Drive/Colab Notebooks/checkpoints'\n","except:\n","    pass\n","\n","try:  # crear directorio para los checkpoints (si no exista ya)\n","      # create directtory for the checkpoints (if it doesn't already exist)\n","    os.makedirs(checkpoint_dir)\n","except FileExistsError:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"8-KGhHrIiDga"},"source":["### Podemos usar un conjunto de datos con un vocabulario muy limitado para facilitar el entrenamiento..."]},{"cell_type":"markdown","metadata":{"id":"-3B-W-KMpSWl"},"source":["### We can use a set of data with a very limited vocabulary to facilitate training..."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1389,"status":"ok","timestamp":1683653394814,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"VZ8u9UdILV1F","outputId":"d583b4a0-b62b-428b-83a1-9d68debd0705"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_en\n","9085267/9085267 [==============================] - 0s 0us/step\n","Downloading data from https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_fr\n","10135742/10135742 [==============================] - 0s 0us/step\n"]}],"source":["path_en = get_file(\n","    'small_vocab_en',\n","    origin=\n","    'https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_en'\n",")\n","path_fr = get_file(\n","    'small_vocab_fr',\n","    origin=\n","    'https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_fr'\n",")\n","\n","with open(path_en, 'rt', encoding='utf-8') as file:\n","    native = [line.strip() for line in file.readlines()]\n","with open(path_fr, 'rt', encoding='utf-8') as file:\n","    foreign = [line.strip() for line in file.readlines()]"]},{"cell_type":"markdown","metadata":{"id":"9ATeqEBx60fA"},"source":["### ...o un conjunto de datos más interesante"]},{"cell_type":"markdown","metadata":{"id":"g6wTgbaYpSWn"},"source":["### ...or a more interesting data set"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1793,"status":"ok","timestamp":1683653396601,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"oLUFNxEwtSmW","outputId":"3fa6f609-217e-4115-cb90-92a489c0faa3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2638744/2638744 [==============================] - 1s 0us/step\n"]}],"source":["# puedes encontrar otros texto paralelos aquí\n","# cuidado porque se actualizan periodicamente\n","# https://www.manythings.org/anki/\n","\n","# you can find other parallel texts here\n","# be careful because they are updated periodically\n","# https://www.manythings.org/anki/\n","\n","path_to_zip = get_file(\n","    'spa-eng.zip',\n","    origin=\n","    'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True)\n","path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\"\n","\n","native = []\n","foreign = []\n","with open(path_to_file, 'rt', encoding='utf-8') as file:\n","    for line in file.readlines():\n","        tab = line.find('\\t')\n","        native += [line[:tab]]\n","        foreign += [line[tab + 1:-1]]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1683653396601,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"dpT3Kl3uS3B1","outputId":"ff9dec4c-8e62-4646-bec1-8a90232abed7"},"outputs":[{"data":{"text/plain":["('Help me!', 'Ayúdame.')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["native[123], foreign[123]"]},{"cell_type":"markdown","metadata":{"id":"dHhzLgFx66ou"},"source":["### Añadir los tokens que marcan el comienzo (\"[SOS]\") y el final (\"[EOS]\") de cada frase\n","\n","¡Tal vez \"[SOS]\" no es una buena elección porque \"sos\" quiere decir \"eres\" en argentina...!"]},{"cell_type":"markdown","metadata":{"id":"1Uld6goFpSWo"},"source":["### Add the tokens that mark the beginning (\"[SOS]\") and the end (\"[EOS]\") of each sentence\n","\n","Maybe \"[SOS]\" is not a good choice because \"sos\" means \"you are\" in Argentina ...!"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683653396602,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"SkkbjjCiWZ6y"},"outputs":[],"source":["foreign = ['[SOS] ' + _ + ' [EOS]' for _ in foreign]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683653396602,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"hQ-UVVTPanUL","outputId":"39126d75-b7fa-4dcf-9bd5-e3f188589c7a"},"outputs":[{"data":{"text/plain":["('Help me!', '[SOS] Ayúdame. [EOS]')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["native[123], foreign[123]"]},{"cell_type":"markdown","metadata":{"id":"kaz65O317eBQ"},"source":["### Tokenizar"]},{"cell_type":"markdown","metadata":{"id":"F6TAb8oSpSWp"},"source":["### Tokenize"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11913,"status":"ok","timestamp":1683653408509,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"OpiysaKEZiES"},"outputs":[],"source":["max_words = 100000  #@param {type : 'number'}\n","\n","native_tokenizer = Tokenizer(num_words=max_words, oov_token='[UNK]')\n","native_tokenizer.fit_on_texts(native)\n","native_index2word = dict(\n","    zip(native_tokenizer.word_index.values(),\n","        native_tokenizer.word_index.keys()))\n","native = native_tokenizer.texts_to_sequences(native)\n","native_vocab_size = min(max_words,\n","                        len(native_tokenizer.index_word.keys()) + 1)  # pad = 0\n","\n","foreign_tokenizer = Tokenizer(num_words=max_words, oov_token='[UNK]')\n","foreign_tokenizer.fit_on_texts(foreign)\n","foreign_index2word = dict(\n","    zip(foreign_tokenizer.word_index.values(),\n","        foreign_tokenizer.word_index.keys()))\n","foreign = foreign_tokenizer.texts_to_sequences(foreign)\n","foreign_vocab_size = min(max_words,\n","                         len(foreign_tokenizer.index_word.keys()) +\n","                         1)  # pad = 0"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1683653408509,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"rdPYRzDRgDzg","outputId":"cce72041-2617-4de2-a430-32e17d914fb0"},"outputs":[{"data":{"text/plain":["(13526, 26937)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["native_vocab_size, foreign_vocab_size"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683653408509,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"9hL2w-qVew3X","outputId":"3310ec53-a904-4c45-824a-b5112c6bd3a3"},"outputs":[{"data":{"text/plain":["([87, 18], [2, 2340, 3])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["native[123], foreign[123]"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1683653408521,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"GWIPwxeYgrMq","outputId":"9b4e9ec9-8846-4510-925e-98de126c1cfc"},"outputs":[{"data":{"text/plain":["(['help', 'me'], ['sos', 'ayúdame', 'eos'])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["[native_index2word.get(_, \"\")\n"," for _ in native[123]], [foreign_index2word.get(_, \"\") for _ in foreign[123]]"]},{"cell_type":"markdown","metadata":{"id":"rUlK4Qqc7h23"},"source":["### Hacer que las secuencias tienen el mismo tamaño"]},{"cell_type":"markdown","metadata":{"id":"PLX4IG9xpSWq"},"source":["### Make the sequences have the same size"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1683653408521,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"58-bb5A2xeGk"},"outputs":[],"source":["import keras"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1404,"status":"ok","timestamp":1683653409907,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"___q1fh9utZg"},"outputs":[],"source":["native = keras.utils.pad_sequences(native, maxlen=20, padding='pre',                   truncating='post')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683653409908,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"1GL1jebLu0YN"},"outputs":[],"source":["foreign  = keras.utils.pad_sequences(foreign, maxlen=20, padding='pre',\n","                                truncating='post')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683653409908,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"3SdLyweMc-w-","outputId":"34d3ada3-1941-4b6b-ad70-08c43dd1bde8"},"outputs":[{"data":{"text/plain":["(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0, 87, 18], dtype=int32),\n"," array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    2, 2340,    3], dtype=int32))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["native[123], foreign[123]"]},{"cell_type":"markdown","metadata":{"id":"rMB0hFpFIyzw"},"source":["### Seq2Seq para NMT (*Neural Machine Translation*)\n","![texto alternativo](https://miro.medium.com/max/1000/1*7Ki7jDc2f_fH1mHluoZKpA.jpeg)\n","https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"]},{"cell_type":"markdown","metadata":{"id":"huP4Vpe0pSWr"},"source":["### Seq2Seq for NMT (*Neural Machine Translation*)\n","![](https://miro.medium.com/max/1000/1*7Ki7jDc2f_fH1mHluoZKpA.jpeg)\n","https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"]},{"cell_type":"markdown","metadata":{"id":"ocnzegSEY1Ga"},"source":["### Capa de Keras para convertir los tokens a vectores one hot"]},{"cell_type":"markdown","metadata":{"id":"mLM90x7rpSWs"},"source":["### Keras layer to convert tokens to one hot vectors"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683653409908,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"wWQQy6fdNvJd"},"outputs":[],"source":["class OneHot(Layer):\n","    def __init__(self, number_of_tokens, **kwargs):\n","        super(OneHot, self).__init__(**kwargs)\n","        self.number_of_tokens = number_of_tokens\n","\n","    def call(self, inputs):\n","        if K.dtype(inputs) != 'int32':\n","            inputs = K.cast(inputs, 'int32')\n","        return K.one_hot(inputs, self.number_of_tokens)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape + (self.number_of_tokens, )"]},{"cell_type":"markdown","metadata":{"id":"DkgKx199ZDiz"},"source":["### Generador para pasar los token objetivos a vectores one hot"]},{"cell_type":"markdown","metadata":{"id":"R33Kkxc8pSWt"},"source":["### Generator to pass target tokens to one hot vectors"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1683653409908,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"hWPApJ0sAEcQ"},"outputs":[],"source":["def batch_generator(x, y, num_tokens, batch_size=128):\n","    indices = np.random.permutation(len(y))\n","    i = 0\n","    while True:\n","        start = i * batch_size\n","        end = start + batch_size\n","        batch = indices[start:end]\n","        batch_X = [x[0][batch], x[1][batch]]\n","        batch_Y = to_categorical(y[batch], num_tokens)\n","        i = i + 1\n","        if end \u003e= len(y):\n","            i = 0\n","        yield (batch_X, batch_Y)"]},{"cell_type":"markdown","metadata":{"id":"uMiv-MQhZj_p"},"source":["### Crear el modelo: el *encoder*"]},{"cell_type":"markdown","metadata":{"id":"Aj0z3BchpSWt"},"source":["### Create the model: the *encoder*"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6952,"status":"ok","timestamp":1683653416854,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"f0M89cvkJwxu"},"outputs":[],"source":["num_hidden_units = 1024  #@param {type : 'number'}\n","embedding_dim = 256  #@param {type : 'number'}\n","\n","encoder_inputs = Input(shape=(20, ), name='encoder_inputs')\n","\n","#encoder_embedding = OneHot(native_vocab_size)\n","encoder_embedding = Embedding(native_vocab_size,\n","                              embedding_dim,\n","                              name='encoder_embedding')\n","encoder_in = encoder_embedding(encoder_inputs)\n","\n","encoder_lstm = LSTM(num_hidden_units,\n","                    return_state=True,\n","                    return_sequences=True,\n","                    name='encoder_lstm')\n","encoder_out, state_h, state_c = encoder_lstm(\n","    encoder_in)  # utilizaremos los output en otro modelo más adelante\n","                 # we will use these outputs in another model later on\n","\n","encoder_states = [state_h, state_c]"]},{"cell_type":"markdown","metadata":{"id":"P6UdUT5tZs7e"},"source":["### el *decoder*"]},{"cell_type":"markdown","metadata":{"id":"Up9Q1n0rpSWu"},"source":["### the *decoder*"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1683653418924,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"F4vJFojEJ1g_"},"outputs":[],"source":["decoder_inputs = Input(\n","    shape=(20 - 1, ),\n","    name='decoder_inputs')  # OJO: secuencia con [SOS] pero sin [EOS]\n","                            # careful: the sequence contains [SOS] but not [EOS]\n","\n","#decoder_embedding = OneHot(foreign_vocab_size)\n","decoder_embedding = Embedding(foreign_vocab_size,\n","                              embedding_dim,\n","                              name='decoder_embedding')\n","decoder_in = decoder_embedding(decoder_inputs)\n","\n","decoder_lstm = LSTM(num_hidden_units,\n","                    return_sequences=True,\n","                    return_state=True,\n","                    name='decoder_lstm')\n","decoder_out, _, _ = decoder_lstm(decoder_in, initial_state=encoder_states)\n","\n","decoder_dense = Dense(foreign_vocab_size, activation='softmax')\n","decoder_outputs = TimeDistributed(decoder_dense, name='decoder_dense')(\n","    decoder_out\n",")  # https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/"]},{"cell_type":"markdown","metadata":{"id":"s3f9P-FFclDH"},"source":["### y el modelo completo"]},{"cell_type":"markdown","metadata":{"id":"YUi75LlHpSWu"},"source":["### and the complete model"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1683653418924,"user":{"displayName":"Carlos Germán Sellers Mendo","userId":"10483787242844398475"},"user_tz":-120},"id":"hRx9OmCqJ5Sr","outputId":"0d23d34e-a9bd-48c8-8475-0198a054f59a"},"outputs":[{"name":"stdout","output_type":"stream","text":["FULL MODEL\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, 20)]         0           []                               \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, 19)]         0           []                               \n","                                                                                                  \n"," encoder_embedding (Embedding)  (None, 20, 256)      3462656     ['encoder_inputs[0][0]']         \n","                                                                                                  \n"," decoder_embedding (Embedding)  (None, 19, 256)      6895872     ['decoder_inputs[0][0]']         \n","                                                                                                  \n"," encoder_lstm (LSTM)            [(None, 20, 1024),   5246976     ['encoder_embedding[0][0]']      \n","                                 (None, 1024),                                                    \n","                                 (None, 1024)]                                                    \n","                                                                                                  \n"," decoder_lstm (LSTM)            [(None, 19, 1024),   5246976     ['decoder_embedding[0][0]',      \n","                                 (None, 1024),                    'encoder_lstm[0][1]',           \n","                                 (None, 1024)]                    'encoder_lstm[0][2]']           \n","                                                                                                  \n"," decoder_dense (TimeDistributed  (None, 19, 26937)   27610425    ['decoder_lstm[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 48,462,905\n","Trainable params: 48,462,905\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['acc'])\n","\n","print('FULL MODEL')\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"zH_oYCCXZxkZ"},"source":["### Entrenar el modelo"]},{"cell_type":"markdown","metadata":{"id":"uyt8VptmpSWv"},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"E-HE6bDbXl_J"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-22-0653f36e52c8\u003e:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  model.fit_generator(batch_generator([native, foreign[:, :-1]],\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","233/233 [==============================] - 365s 2s/step - loss: 2.8999 - acc: 0.6187\n","Epoch 2/25\n","233/233 [==============================] - 338s 1s/step - loss: 2.2688 - acc: 0.6481\n","Epoch 3/25\n","233/233 [==============================] - 338s 1s/step - loss: 2.2086 - acc: 0.6544\n","Epoch 4/25\n","233/233 [==============================] - 335s 1s/step - loss: 2.1790 - acc: 0.6559\n","Epoch 5/25\n","233/233 [==============================] - 336s 1s/step - loss: 2.1565 - acc: 0.6571\n","Epoch 6/25\n","233/233 [==============================] - 336s 1s/step - loss: 2.1379 - acc: 0.6577\n","Epoch 7/25\n","233/233 [==============================] - 333s 1s/step - loss: 2.1216 - acc: 0.6586\n","Epoch 8/25\n","233/233 [==============================] - 345s 1s/step - loss: 2.1061 - acc: 0.6592\n","Epoch 9/25\n","233/233 [==============================] - 336s 1s/step - loss: 2.0882 - acc: 0.6604\n","Epoch 10/25\n","233/233 [==============================] - 337s 1s/step - loss: 2.0672 - acc: 0.6620\n","Epoch 11/25\n","233/233 [==============================] - 338s 1s/step - loss: 2.0448 - acc: 0.6638\n","Epoch 12/25\n","233/233 [==============================] - 337s 1s/step - loss: 2.0232 - acc: 0.6655\n","Epoch 13/25\n","233/233 [==============================] - 334s 1s/step - loss: 2.0031 - acc: 0.6670\n","Epoch 14/25\n","233/233 [==============================] - 335s 1s/step - loss: 1.9838 - acc: 0.6683\n","Epoch 15/25\n","233/233 [==============================] - 336s 1s/step - loss: 1.9662 - acc: 0.6694\n","Epoch 16/25\n","222/233 [===========================\u003e..] - ETA: 15s - loss: 1.9486 - acc: 0.6707"]}],"source":["#model.load_weights(checkpoint_dir + '/Seq2SeqModel.h5')\n","batch_size = 512  #@param {type : 'number'}\n","model.fit_generator(batch_generator([native, foreign[:, :-1]],\n","                                    foreign[:, 1:],\n","                                    foreign_vocab_size,\n","                                    batch_size=batch_size),\n","                    steps_per_epoch=native.shape[0] // batch_size + 1,\n","                    epochs=25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrISaQiEUuOG"},"outputs":[],"source":["model.save_weights(checkpoint_dir + '/Seq2SeqModel.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iN9Dr0gQbqHL"},"outputs":[],"source":["model.load_weights(checkpoint_dir + '/Seq2SeqModel.h5')"]},{"cell_type":"markdown","metadata":{"id":"KWDdmefFYtQo"},"source":["### Visualizar los embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqX-ij8iYIL3"},"outputs":[],"source":["def plot_tsne(embedding_matrix, index_to_word, num_words=None, title='t-SNE'):\n","    matrix = embedding_matrix\n","    if num_words is not None:\n","        matrix = matrix[:num_words, :]\n","    labels = [index_to_word.get(_, \"\u003cUNK\u003e\") for _ in range(matrix.shape[0])]\n","\n","    # defining the chart\n","    output_notebook()\n","    fig = bp.figure(plot_width=700,\n","                    plot_height=600,\n","                    title=title,\n","                    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n","                    x_axis_type=None,\n","                    y_axis_type=None,\n","                    min_border=1)\n","\n","    # dimensionality reduction. converting the vectors to 2d vectors\n","    tsne_model = TSNE(n_components=2, verbose=0, random_state=0)\n","    tsne_w2v = tsne_model.fit_transform(matrix)\n","\n","    # putting everything in a dataframe\n","    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n","    tsne_df['words'] = labels\n","\n","    # plotting. the corresponding word appears when you hover on the data point.\n","    fig.scatter(x='x', y='y', source=tsne_df)\n","    hover = fig.select(dict(type=HoverTool))\n","    hover.tooltips = {\"\": \"@words\"}\n","    show(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKDx0RYiXIoM"},"outputs":[],"source":["plot_tsne(encoder_embedding.get_weights()[0],\n","          native_index2word,\n","          title='English')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejX20qTlYg3M"},"outputs":[],"source":["plot_tsne(decoder_embedding.get_weights()[0],\n","          foreign_index2word,\n","          title='Spanish')"]},{"cell_type":"markdown","metadata":{"id":"7wb7gAEVWouZ"},"source":["### *Teacher Forcing*\n","\n","\n","\n","``model_inputs = [native, foreign[:,:-1]], model_outputs = foreign[:, 1:]\n","``"]},{"cell_type":"markdown","metadata":{"id":"YEmdm9lupSWy"},"source":["### Teacher Forcing\n","\n","\n","\n","`` model_inputs = [native, foreign[:,:-1]], model outputs = foreign[:, 1:]\n","``"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goRmHCTLu6E8"},"outputs":[],"source":["for i in range(10):\n","    ind = random.randint(0, len(native))\n","    sentence = (' ').join([native_index2word.get(_, '') for _ in native[ind]])\n","    target_sentence = (' ').join(\n","        [foreign_index2word.get(_, '') for _ in foreign[ind][1:]])\n","    seq = np.expand_dims(native[ind], axis=0)\n","\n","    # empieza bien, pero rápidamente pierde el hilo\n","    # it starts off well, but it quickly loses the thread\n","    dec_inputs = np.zeros((1, max_len - 1))\n","    dec_inputs[0, 0] = foreign_tokenizer.word_index['sos']\n","    dec_outputs = model.predict([seq, dec_inputs])\n","    tokens = np.argmax(dec_outputs, axis=-1)[0]\n","    translated_sentence = ' '.join(\n","        [foreign_index2word.get(_, '') for _ in tokens])\n","\n","    # esto va mejor, pero es hacer \"trampa\"...\n","    # this works better, but it is \"cheating\"...\n","    dec_inputs = np.expand_dims(foreign[ind, :-1], axis=0)\n","    dec_outputs = model.predict([seq, dec_inputs])\n","    tokens = np.argmax(dec_outputs, axis=-1)[0]\n","    translated_sentence_with_teacher_forcing = ' '.join(\n","        [foreign_index2word.get(_, '') for _ in tokens])\n","\n","    print(sentence)\n","    print(translated_sentence)\n","    print(translated_sentence_with_teacher_forcing)\n","    print(target_sentence)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzENBD4GJ8wd"},"outputs":[],"source":["encoder_inf_inputs = Input((max_len, ), name='encoder_inf_inputs')\n","\n","encoder_inf_in = encoder_embedding(encoder_inf_inputs)\n","\n","encoder_inf_out, state_h, state_c = encoder_lstm(encoder_inf_in)\n","\n","encoder_model = Model(inputs=encoder_inf_inputs,\n","                      outputs=[encoder_inf_out, state_h, state_c])\n","\n","print('ENCODER')\n","encoder_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVX8Z5wm4mJq"},"outputs":[],"source":["decoder_init_states = [\n","    Input((num_hidden_units, ), name='encoder_state_h'),\n","    Input((num_hidden_units, ), name='encoder_state_c')\n","]\n","encoder_inf_outputs = Input((max_len, num_hidden_units),\n","                            name='encoder_inf_outputs')\n","decoder_inf_inputs = Input((1, ), name='decoder_inf_inputs')  # token por token\n","\n","decoder_inf_in = decoder_embedding(decoder_inf_inputs)\n","\n","decoder_inf_out, state_h, state_c = decoder_lstm(\n","    decoder_inf_in, initial_state=decoder_init_states)\n","\n","decoder_inf_pred = TimeDistributed(decoder_dense,\n","                                   name='decoder_inf_dense')(decoder_inf_out)\n","\n","decoder_model = Model(inputs=[encoder_inf_outputs] + decoder_init_states +\n","                      [decoder_inf_inputs],\n","                      outputs=[decoder_inf_pred, state_h, state_c])\n","\n","print('DECODER')\n","decoder_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgOnVCQcpgy6"},"outputs":[],"source":["# phrase in english\n","frase_en_ingles = \"Who did you go to the movie theater with?\"  #@param {type : 'string'}\n","seq = native_tokenizer.texts_to_sequences([frase_en_ingles])\n","seq = sequence.pad_sequences(seq,\n","                             maxlen=max_len,\n","                             padding='pre',\n","                             truncating='post')\n","sentence = (' ').join([native_index2word.get(_, '') for _ in seq[0]])\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0tGUz-QfQo7"},"outputs":[],"source":["enc_outputs, h, c = encoder_model.predict(seq)\n","dec_inputs = np.expand_dims(foreign_tokenizer.word_index['sos'], axis=0)\n","translated_sentence = ''\n","for i in range(max_len):\n","    dec_outputs, h, c = decoder_model.predict([enc_outputs, h, c, dec_inputs])\n","    token = np.argmax(dec_outputs, axis=-1)[0, 0]\n","    word = foreign_index2word.get(token, '')\n","    if word == '' or word == 'eos':\n","        break\n","    dec_inputs = np.expand_dims(token, axis=0)\n","    translated_sentence += word + ' '\n","print(translated_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4vgK67KVPiZ"},"outputs":[],"source":["k = 5\n","enc_outputs, h, c = encoder_model.predict(seq)\n","candidates = [('', 0, foreign_tokenizer.word_index['sos'])]\n","candidates  # (frase hasta ahora, probabilidad de la última palabra condicionada en la previa, token de la última palabra)\n","            # (phrase up to this point, probability of the last word conditioned on the previous one, last word token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNQLahpCW1kz"},"outputs":[],"source":["dec_inputs = np.array([[_[2]] for _ in candidates])\n","dec_outputs, h, c = decoder_model.predict([enc_outputs, h, c, dec_inputs])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4rlSLqMW_Ua"},"outputs":[],"source":["top_k = [\n","    np.flip(dec_outputs[token, 0].argsort()[-k:])\n","    for token in range(min(k, len(candidates)))\n","]\n","top_k  # los siguientes token más probables\n","       # the next most likely"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yE2HToQqXYXz"},"outputs":[],"source":["next_candidates = [\n","    (\n","        candidates[token][0] + ' ' + foreign_index2word.get(\n","            top_k[token][next_token], ''),  # frase hasta ahora\n","                                            # phrase up this point\n","        candidates[token][1] - np.log(\n","            dec_outputs[token, 0, top_k[token][next_token]]\n","        ),  # probabilidad de la última palabra condicionada en la previa\n","            # probability of the last word conditioned on the previous one\n","        top_k[token][next_token])  # token de la última palabra\n","                                   # last word token\n","    for next_token in range(k) for token in range(len(top_k))\n","]\n","candidates = sorted(next_candidates, key=lambda x: x[1])[:k]\n","candidates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeK4hOp9YeUI"},"outputs":[],"source":["# preparar batch para predicir las siguientes palabras condicionadas en estas\n","# prepare a batch to predict the next words conditioned on these ones\n","dec_inputs = np.array([[_[2]] for _ in candidates])\n","dec_inputs.shape, enc_outputs.shape, h.shape, c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH6MC3iWYpcb"},"outputs":[],"source":["# igualar los tamaños de los inputs al modelo\n","# make all the inputs have the same sizes\n","h = np.repeat(h, dec_inputs.shape[0] - h.shape[0] + 1, axis=0)\n","c = np.repeat(c, dec_inputs.shape[0] - c.shape[0] + 1, axis=0)\n","enc_outputs = np.repeat(enc_outputs,\n","                        dec_inputs.shape[0] - enc_outputs.shape[0] + 1,\n","                        axis=0)\n","dec_inputs.shape, enc_outputs.shape, h.shape, c.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3vImpecYw7e"},"outputs":[],"source":["# repetir para la siguiente palabra\n","# repeat for the next word\n","dec_outputs, h, c = decoder_model.predict([enc_outputs, h, c, dec_inputs])\n","top_k = [\n","    np.flip(dec_outputs[token, 0].argsort()[-k:])\n","    for token in range(min(k, len(candidates)))\n","]\n","next_candidates = [\n","    (\n","        candidates[token][0] + ' ' + foreign_index2word.get(\n","            top_k[token][next_token], ''),  # frase hasta ahora\n","                                            # phrase up this point\n","        candidates[token][1] - np.log(\n","            dec_outputs[token, 0, top_k[token][next_token]]\n","        ),  # probabilidad de la última palabra condicionada en la previa\n","            # probability of the last word conditioned on the previous one\n","        top_k[token][next_token])  # token de la última palabra\n","                                   # last word token\n","    for next_token in range(k) for token in range(len(top_k))\n","]\n","sorted(next_candidates, key=lambda x: x[1])"]},{"cell_type":"markdown","metadata":{"id":"VgjGYc20Y8oi"},"source":["### Juntamos las piezas"]},{"cell_type":"markdown","metadata":{"id":"Jaf0pagzpSW3"},"source":["### We put the pieces together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eYzzqkEWHzP"},"outputs":[],"source":["k = 40  #@param {type : 'number'}\n","enc_outputs, h, c = encoder_model.predict(seq)\n","candidates = [('', 0, foreign_tokenizer.word_index['sos'])]\n","dec_inputs = np.array([[_[2]] for _ in candidates])\n","for i in range(max_len):\n","    h = np.repeat(h, dec_inputs.shape[0] - h.shape[0] + 1, axis=0)\n","    c = np.repeat(c, dec_inputs.shape[0] - c.shape[0] + 1, axis=0)\n","    enc_outputs = np.repeat(enc_outputs,\n","                            dec_inputs.shape[0] - enc_outputs.shape[0] + 1,\n","                            axis=0)\n","    dec_outputs, h, c = decoder_model.predict([enc_outputs, h, c, dec_inputs])\n","    top_k = [\n","        np.flip(dec_outputs[token, 0].argsort()[-k:])\n","        for token in range(min(k, len(candidates)))\n","    ]\n","    next_candidates = [\n","        (\n","            candidates[token][0] + ' ' + foreign_index2word.get(\n","                top_k[token][next_token], ''),  # frase hasta ahora\n","                                                # phrase up this point\n","            candidates[token][1] - np.log(\n","                dec_outputs[token, 0, top_k[token][next_token]]\n","            ),  # probabilidad de la última palabra condicionada en la previa\n","                # probability of the last word conditioned on the previous one\n","            top_k[token][next_token])  # token de la última palabra\n","                                       # last word token\n","        for next_token in range(k) for token in range(len(top_k))\n","    ]\n","    candidates = sorted(next_candidates, key=lambda x: x[1])[:k]\n","    print(candidates[0][0])\n","    #    if candidates[0][2] == 0 or candidates[0][2] == foreign_tokenizer.word_index['eos']:\n","    #        break\n","    dec_inputs = np.array([[_[2]] for _ in candidates])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHkE8F5-X4ec"},"outputs":[],"source":["for i, _ in enumerate(candidates):\n","    print(f'{i+1}.{_[0]}')"]},{"cell_type":"markdown","metadata":{"id":"6J9sxe_cyfdC"},"source":["# *Attention*\n","![](https://miro.medium.com/max/700/1*TkVKlV-Pk7POJUhlWHRgDA.png)\n","![](https://miro.medium.com/max/700/1*wcxAAgQ0n9gOXLRqhmaLGA.png)\n","\n","https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n","*  Los LSTM (*Long Short Term Memory*) cuentan con un mecanismo para \"recordar\" elementos previos de una serie. Para traducir un texto, esto es fundamental porque el contexto influye al significado de una palabra. Sin embargo, a veces no sabemos la importancia de una palabra en una frase hasta más adelante y, en la práctica, tenemos que volver a mirar el texto. Por ejemplo en la frase \"*The cat didn't cross the road because it was tired*\", sabemos que es la gata que está cansada, no la calle y, por lo tanto, la traducción a español es \"*El gato no quería cruzar la calle porque estaba cansado*\" y no \"*El gato no quería cruzar la calle porque estaba* cansada\". El mecanismo de atención puede aprender a asociar \"*tired*\" con \"*cat*\" más que con \"*street*\".\n","*  [Existen muchas diferentes formas de calcular \"*attention*\"](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html). Vamos a usar el mismo tipo de atención que se usa en el *transformer* en que está basado BERT.\n","\n","$$\\text{Weights}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\cdot\\mathbf{K}^\\top}{\\sqrt{N}}\\right)$$\n","$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K},\\mathbf{V}) = \\text{Weights}\\cdot\\mathbf{V}$$\n","$$ $$\n","$$\\mathbf{Q}\\in\\rm I\\!R^{n\\times N},\\mathbf{K},\\mathbf{V}\\in\\rm I\\!R^{m\\times N}$$\n","$$\\mathbf{Weights}\\in\\rm I\\!R^{n\\times m},\\mathbf{Attention}\\in\\rm I\\!R^{n\\times N}$$\n","\n","*  Calculamos la atención con *scaled dot product attention*. El dot product es equivalente a la proximidad cosena entre dos vectores.\n","*  La clave $K$ (\"*the key*\") es una matriz de $m$ vectores de $N$ estados, la pregunta $Q$ (\"*the query*\") es una matriz de $n$ vectores de $N$ estados y el valor (\"*the value*\" $V$) es una matriz de $m$ vectores de $N$ estados. Para nuestro propósito $V=K$.\n","*  $\\mathbf{Q}$, $\\mathbf{K}$ y $\\mathbf{V}$ son los resultados de multiplicar $Q$, $K$ y $V$ por una matriz de pesos entrenables (una capa dense) del tamaño correspondente.\n","*  La clave $K$ va a ser la salida del LSTM del encoder y la pregunta $Q$ va a ser la salida del LSTM del decoder.\n","*  Podemos pasar la salida de atención al LSTM o directamente a la capa dense del decoder. El primer enfoque requiere que pasemos la salida de atención a cada célula del LSTM en serie, mientras el segundo enfoque es más fácil de implementar en Keras ya que podemos simplemente concatenar la matriz la salida de atención con la salida del LSTM. [Aquí](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention) hay una implementación del primer enfoque en TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"VJvDlcjtpSW4"},"source":["# *Attention*\n","![](https://miro.medium.com/max/700/1*TkVKlV-Pk7POJUhlWHRgDA.png)\n","![](https://miro.medium.com/max/700/1*wcxAAgQ0n9gOXLRqhmaLGA.png)\n","\n","https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n","*   The LSTM (*Long Short Term Memory*) have a mechanism to \"remember\" previous elements of a series. To translate a text, this is fundamental because the context influences the meaning of a word. However, sometimes we don't know the importance of a word in a sentence until later and, in practice, we have to look back at the text. For example, in the phrase \"*The cat didn't cross the road because it was tired*\", we know that it is the cat that is tired, not the street and, therefore, the Spanish translation is \"*El gato no quería cruzar la calle porque estaba cansado*\" and not \"*El gato no quería cruzar la calle porque estaba* cansada\". The attention mechanism can learn to associate \"*tired*\" with \"*cat*\" more than with \"*street*\". [There are many different ways to calculate \"*attention*\"](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html). We will use the same type of attention that is used in the *transformer* on which BERT is based.\n","\n","$$\\text{Weights}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\cdot\\mathbf{K}^\\top}{\\sqrt{N}}\\right)$$\n","$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K},\\mathbf{V}) = \\text{Weights}\\cdot\\mathbf{V}$$\n","$$ $$\n","$$\\mathbf{Q}\\in\\rm I\\!R^{n\\times N},\\mathbf{K},\\mathbf{V}\\in\\rm I\\!R^{m\\times N}$$\n","$$\\mathbf{Weights}\\in\\rm I\\!R^{n\\times m},\\mathbf{Attention}\\in\\rm I\\!R^{n\\times N}$$\n","\n","*   We calculate attention with *scaled dot product attention*. The dot product is equivalent to the cosine proximity between two vectors. The key $K$ (\"*the key*\") is an array of $m$ vectors of $N$ states, the question $Q$ (\"*the query*\") is an array of $n$ vectors of $N$ states and the value (\"*the value*\" $V$) is an array of $m$ vectors of $N$ states. For our purposes $V=K$.\n","*   $\\mathbf{Q}$, $\\mathbf{K}$ and $\\mathbf{V}$ are the results of multiplying $Q$, $K$ and $V$ by a matrix of trainable weights (a dense layer) of the corresponding size. The key $K$ is going to be the LSTM output of the encoder and the query $Q$ is going to be the LSTM output of the decoder.\n","*   We can pass the attention output to the LSTM or directly to the dense layer of the decoder. The first approach requires that we pass the attention output to each LSTM cell in series, while the second approach is easier to implement in Keras since we can simply concatenate the attention output matrix with the LSTM output. [Here](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention) is an implementation of the first approach in TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"p0uUeQzIjliQ"},"source":["### Redefinimos el modelo para incluir una capa de atención"]},{"cell_type":"markdown","metadata":{"id":"BNoirCzupSW4"},"source":["### We redefine the model to include an attention layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZI37Gw0xCxc5"},"outputs":[],"source":["# adaptado de https://github.com/tensorflow/models/blob/master/official/transformer/model/attention_layer.py\n","\n","\n","class Attention(Layer):\n","    def __init__(self, hidden_size, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","        self.hidden_size = hidden_size\n","\n","        # capas dense para los query, key y values\n","        # Layers for linearly projecting the queries, keys, and values.\n","        self.q_dense_layer = Dense(hidden_size, use_bias=False, name=\"q\")\n","        self.k_dense_layer = Dense(hidden_size, use_bias=False, name=\"k\")\n","        self.v_dense_layer = Dense(hidden_size, use_bias=False, name=\"v\")\n","        self.output_dense_layer = Dense(hidden_size,\n","                                        use_bias=False,\n","                                        name=\"output_transform\")\n","\n","    def build(self, input_shape):\n","        # construir las capas\n","        # build the layers\n","        self.q_dense_layer.build(input_shape[1])\n","        self.k_dense_layer.build(input_shape[0])\n","        self.v_dense_layer.build(input_shape[0])\n","        self.output_dense_layer.build(input_shape[0])\n","\n","        # añadir los pesos entrenables\n","        # add the trainable weights\n","        self.trainable_weights = self.q_dense_layer.trainable_weights\n","        self.trainable_weights += self.k_dense_layer.trainable_weights\n","        self.trainable_weights += self.v_dense_layer.trainable_weights\n","        self.trainable_weights += self.output_dense_layer.trainable_weights\n","        super(Attention, self).build(input_shape)\n","\n","    def call(self, inputs, bias=0):\n","        \"\"\"Apply attention mechanism to inputs.\n","        Args:\n","          inputs[0]: a tensor with shape [batch_size, length_encoder, hidden_size]\n","          inputs[1]: a tensor with shape [batch_size, length_decoder, hidden_size]\n","          bias: attention bias that will be added to the result of the dot product.\n","        Returns:\n","          Attention layer output with shape [batch_size, length_decoder, hidden_size]\n","          Attention weights with shape [batch_size, length_decoder, length_encoder]\n","        \"\"\"\n","        q = self.q_dense_layer(inputs[1])\n","        k = self.k_dense_layer(inputs[0])\n","        v = self.v_dense_layer(inputs[0])\n","\n","        # escalar q para evitar problemas con calcular el producto escalar de q y k\n","        # Scale q to prevent the dot product between q and k from growing too large.\n","        depth = self.hidden_size\n","        q *= depth**-0.5\n","\n","        # calulcuar atención con el producto escalar\n","        # Calculate dot product attention\n","        logits = K.batch_dot(q, K.permute_dimensions(k, [0, 2, 1]))\n","        logits += bias\n","        weights = K.softmax(logits)\n","        attention_output = K.batch_dot(weights, v)\n","\n","        # pasar los outputs por otra capa dense\n","        # Run the combined outputs through another linear projection layer.\n","        attention_output = self.output_dense_layer(attention_output)\n","        return [attention_output, weights]\n","\n","    def compute_output_shape(self, input_shape):\n","        return [(input_shape[0][0], input_shape[1][1], self.hidden_size),\n","                (input_shape[1][0], input_shape[1][1], input_shape[0][1])]"]},{"cell_type":"markdown","metadata":{"id":"93zY7jXYjrXf"},"source":["### Conectamos las salidas del encoder y el decoder a una capa de atención..."]},{"cell_type":"markdown","metadata":{"id":"Cu53FsvspSW5"},"source":["### We connect the encoder and decoder outputs to an attention layer..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRKWBtQct2mk"},"outputs":[],"source":["attention_hidden_size = 1024  #@param {type : 'number'}\n","\n","attn_layer = Attention(attention_hidden_size, name='attention')\n","attn_out, attn_weights = attn_layer([encoder_out, decoder_out])\n","\n","decoder_cat = Concatenate(name='concatenate')\n","decoder_merge = decoder_cat([decoder_out, attn_out])"]},{"cell_type":"markdown","metadata":{"id":"G_ar1BT7zqv5"},"source":["### ...y las salidas de la capa de atención a la capa dense del decoder"]},{"cell_type":"markdown","metadata":{"id":"uuKdiHaxpSW6"},"source":["### ...and the outputs of the attention layer to the decoder layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzGJbyIQt32L"},"outputs":[],"source":["decoder_dense = Dense(foreign_vocab_size, activation='softmax')\n","decoder_outputs = TimeDistributed(decoder_dense,\n","                                  name='decoder_dense')(decoder_merge)"]},{"cell_type":"markdown","metadata":{"id":"ZbFAWo2GjG3j"},"source":["### El modelo completo"]},{"cell_type":"markdown","metadata":{"id":"ahDDGy5qpSW6"},"source":["### The complete model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXV8aEkkuCQs"},"outputs":[],"source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['acc'])\n","\n","print('FULL MODEL')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtlPgk9e0y7k"},"outputs":[],"source":["#model.load_weights(checkpoint_dir + '/Seq2SeqAttnModel.h5')\n","batch_size = 512  #@param {type : 'number'}\n","model.fit_generator(batch_generator([native, foreign[:, :-1]],\n","                                    foreign[:, 1:],\n","                                    foreign_vocab_size,\n","                                    batch_size=batch_size),\n","                    steps_per_epoch=native.shape[0] // batch_size + 1,\n","                    epochs=25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzVtKxqF1HOA"},"outputs":[],"source":["model.save_weights(checkpoint_dir + '/Seq2SeqAttnModel.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u62CdolT4rFn"},"outputs":[],"source":["model.load_weights(checkpoint_dir + '/Seq2SeqAttnModel.h5')"]},{"cell_type":"markdown","metadata":{"id":"tutdCox9vwuW"},"source":["### Añadimos la capa de atención al decoder para hacer inferencia"]},{"cell_type":"markdown","metadata":{"id":"fILzWefVpSW9"},"source":["### We add the attention layer to the decoder for inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7mZ11DfCZJg"},"outputs":[],"source":["attn_out, attn_weights = attn_layer([encoder_inf_outputs, decoder_inf_out])\n","\n","decoder_inf_merge = decoder_cat([decoder_inf_out, attn_out])\n","\n","decoder_inf_pred = TimeDistributed(decoder_dense,\n","                                   name='decoder_inf_dense')(decoder_inf_merge)\n","\n","decoder_model = Model(\n","    inputs=[encoder_inf_outputs] + decoder_init_states + [decoder_inf_inputs],\n","    outputs=[decoder_inf_pred, state_h, state_c, attn_weights])\n","\n","print('DECODER')\n","decoder_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"JWFeNigcwlXb"},"source":["### Beam search con atención"]},{"cell_type":"markdown","metadata":{"id":"VriZeSbypSW-"},"source":["### Beam search with attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMpog5ktwkBK"},"outputs":[],"source":["frase_en_ingles = \"Who did you go to the movie theater with?\"  #@param {type : 'string'}\n","seq = native_tokenizer.texts_to_sequences([frase_en_ingles])\n","seq = sequence.pad_sequences(seq,\n","                             maxlen=max_len,\n","                             padding='pre',\n","                             truncating='post')\n","sentence = (' ').join([native_index2word.get(_, '') for _ in seq[0]])\n","print(sentence)\n","\n","k = 40  #@param {type : 'number'}\n","enc_outputs, h, c = encoder_model.predict(seq)\n","candidates = [('', 0, foreign_tokenizer.word_index['sos'], [])]\n","dec_inputs = np.array([[_[2]] for _ in candidates])\n","for i in range(max_len):\n","    h = np.repeat(h, dec_inputs.shape[0] - h.shape[0] + 1, axis=0)\n","    c = np.repeat(c, dec_inputs.shape[0] - c.shape[0] + 1, axis=0)\n","    enc_outputs = np.repeat(enc_outputs,\n","                            dec_inputs.shape[0] - enc_outputs.shape[0] + 1,\n","                            axis=0)\n","    dec_outputs, h, c, attn = decoder_model.predict(\n","        [enc_outputs, h, c, dec_inputs])\n","    top_k = [\n","        np.flip(dec_outputs[token, 0].argsort()[-k:])\n","        for token in range(min(k, len(candidates)))\n","    ]\n","    next_candidates = [\n","        (\n","            candidates[token][0] + ' ' + foreign_index2word.get(\n","                top_k[token][next_token], ''),  # frase hasta ahora\n","                                                # phrase up to this point\n","            candidates[token][1] - np.log(\n","                dec_outputs[token, 0, top_k[token][next_token]]\n","            ),  # probabilidad de la última palabra condicionada en la previa\n","                # probability of the last word conditioned on the previous one\n","            top_k[token][next_token],  # token de la última palabra\n","                                       # last word token\n","            candidates[token][3] + [attn[token]])  # vectores de atención\n","                                                   # attention vectors\n","        for next_token in range(k) for token in range(len(top_k))\n","    ]\n","    candidates = sorted(next_candidates, key=lambda x: x[1])[:k]\n","    #    if candidates[0][2] == 0 or candidates[0][2] == foreign_tokenizer.word_index['eos']:\n","    #        break\n","    dec_inputs = np.array([[_[2]] for _ in candidates])\n","attention_matrix = np.concatenate(candidates[0][3], axis=0)\n","for i, _ in enumerate(candidates):\n","    print(f'{i+1}.{_[0]}')"]},{"cell_type":"markdown","metadata":{"id":"UqTFTYph4PnY"},"source":["### Visualizar la matriz de atención"]},{"cell_type":"markdown","metadata":{"id":"nFBVX2HlpSW-"},"source":["### Display the attention matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWmVjyYp00CR"},"outputs":[],"source":["index = candidates[0][0].split()\n","columns = sentence.split()\n","df_cm = pd.DataFrame(attention_matrix[:len(index), -len(columns):],\n","                     index=index,\n","                     columns=columns)\n","fig = plt.figure(figsize=(10, 7))\n","fig.suptitle('Attention matrix', fontsize=24)\n","sn.heatmap(df_cm)"]},{"cell_type":"markdown","metadata":{"id":"vm3CYC3F7kXV"},"source":["### *Attention is all you need*\n","\n","En 2017, Google publicó un artículo muy influyente con el título provocador \"[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\" o, sólo hace falta atención. Consiguieron muy buenos resultados, batiendo algunos records, sólo con atención y sin ninguna RNN (LSTM, GRU, etc) ni convolución. Una de las innovaciones clave fue un encoder de la posición de cada palabra (\"[positional encoding](https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/)\"), lo que permite tratar frases enteras de manera global en lugar de palabra por palabra. De esta manera, evitan el problema de los \"gradientes desvanecientes\" (*vanishing gradients*) de los RNNs con secuencias largas. El bloque de capas que constituye el encoder y el decoder se llama un [transformer](https://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py).\n","![](https://lilianweng.github.io/lil-log/assets/images/transformer.png)"]},{"cell_type":"markdown","metadata":{"id":"hivwHJiWpSW-"},"source":["### Attention is all you need\n","\n","In 2017, Google published a very influential article with the provocative title \"[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\" or , it only needs attention. They achieved very good results, beating some records, only with attention and without any RNN (LSTM, GRU, etc.) or convolution. One of the key innovations was an encoder of the position of each word (\"[positional encoding](https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/)\"), what which allows you to treat whole sentences globally instead of word for word. In this way, they avoid the problem of the \"vanishing gradients\" of RNNs with long sequences. The layer block that constitutes the encoder and decoder is called a [transformer](https://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py).\n","![](https://lilianweng.github.io/lil-log/assets/images/transformer.png)"]},{"cell_type":"markdown","metadata":{"id":"PDbPKKCAfiLs"},"source":["https://www.tensorflow.org/beta/tutorials/text/transformer"]},{"cell_type":"markdown","metadata":{"id":"lGW_FPxVluwG"},"source":["### TODO\n","*  Explain dot product attention better with an example\n","*  Use StackedRNNCells to make AttentionLSTM layer\n","*  Explain BERT, Positional Encoding, Multi-Head Attention, Feed Forward, Self-Attention, masking, training, fine-tune discrepancy \n","*  Explain XLNet, AE and AR models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjLSxhECpSW_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nbTranslate":{"displayLangs":["es"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"es","targetLang":"en","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}